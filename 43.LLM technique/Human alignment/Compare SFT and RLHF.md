## 对比SFT和RLHF
从本质上说，SFT 所采用的这种**词元级别**的训练方式**是一种“行为克隆”。**
本质上来说，为了学习教师的生成策略，SFT 采用了基于示例数据的“**局部**”优化方式，即**词元级别的损失函数**。作为对比，RLHF 则采用了涉及人类偏好的“**全局**”优化方式，即**文本级别的损失函数**。
### SFT

#### 优点
1、提高大语言模型在各种基准测试中的性能
2、增强大语言模型在不同任务上的泛化能力
3、提升大语言模型在专业领域的能力
#### 缺点
1、**当数据超出大语言模型的知识范围时，模型易产生幻觉**
2、通过对教师模型的蒸馏，会增加学生模型出现幻觉的可能性
3、**不同标注者对实例数据标注的差异，会影响 SFT 的学习性能**
4、指令数据的质量会影响大语言模型的训练效果
### RLHF
#### 优点
1、进一步增强模型的能力，提高模型有用性
2、有效减轻大语言模型出现有害响应的可能性
3、有效**减轻大语言模型出现幻觉的可能性**
4、偏好标注可以减轻示例生成过程中的不一致情况
#### 缺点
1、训练样本使用效率较低
2、训练过程不稳定，训练过程对**超参数敏感**
3、依赖强大的 SFT 模型进行**热启动**