## 对齐数据的收集
在大语言模型与人类偏好的对齐训练过程中，如何构造高质量的对齐数据集是一个关键问题。

### 基于奖励模型的方法
在 RLHF 方法中，由于奖励模型已经在包含人类偏好的反馈数据集上进行了训练，因此可以将训练好的奖励模型用于评估大语言模型输出的对齐程度。
具体来说，大语言模型首先基于输入生成相应的输出，然后**奖励模型对其输出进行打分，按照分数可以将这些输入与输出划分到不同的组**，因此便可以得到与人类偏好具有不同对齐水平的数据，可用于后续的监督微调。此外，对于**基于排序式反馈数据训练的奖励模型，可以利用奖励模型对大语言模型的多个输出进行质量高低排序**.

### 基于大语言模型的方法
利用大语言模型自身的能力来自动生成与人类价值观对齐的数据成为了一个可以探索的研究问题。Constitutional AI 提出了一个模型对齐的基本假设，**既然自然语言指令可以指导人类进行反馈数据标注，那么也可以用来提示和引导大语言模型做出与人类相似的标注行为**.
可以编写符合人类对齐标准的自然语言指令与相关示例，进而**让大语言模型对其输出进行自我评价与检查**，并针对有害内容进行**迭代式修正**，最终生成与人类价值观对齐的数据集。
示例：
![alt text](image-12.png)
